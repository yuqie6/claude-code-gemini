# Required: Your Gemini API key
GEMINI_API_KEY="your-gemini-api-key-here"

# Optional: Gemini API base URL (default: https://generativelanguage.googleapis.com)
# You can change this to use custom endpoints or proxy services
GEMINI_BASE_URL="https://generativelanguage.googleapis.com"

# Optional: Model mappings (BIG and SMALL models)
BIG_MODEL="gemini-2.5-pro"
# Used for Claude sonnet/opus requests - high capability model
SMALL_MODEL="gemini-2.5-flash"
# Used for Claude haiku requests - fast and efficient model

# Optional: Server settings
HOST="0.0.0.0"
PORT="8082"
LOG_LEVEL="INFO"
# DEBUG, INFO, WARNING, ERROR, CRITICAL

# Optional: Performance settings
MAX_TOKENS_LIMIT="4096"
# Minimum tokens limit for requests (to avoid errors with thinking model)
MIN_TOKENS_LIMIT="4096"
REQUEST_TIMEOUT="90"
MAX_RETRIES="2"

# Optional: Thinking configuration for Gemini 2.5 models
# Enable these to use Gemini's thinking capabilities (thought summaries, reasoning)
BIG_MODEL_THINKING_BUDGET="-1"        # -1 for automatic dynamic thinking (recommended for Pro models)
SMALL_MODEL_THINKING_BUDGET="10000"   # Fixed budget for Flash models (cost control)
ENABLE_THINKING_BY_DEFAULT="true"     # Enable thinking by default when not specified in request

# Optional: Content caching to reduce token usage
# WARNING: Caching may reduce LLM effectiveness by shortening context
ENABLE_CONTENT_CACHE="false"          # Enable client-side content caching (default: false)
CACHE_MIN_CHARS="4096"                # Minimum characters to trigger caching (default: 4096)
CACHE_TTL_HOURS="24"                  # Cache time-to-live in hours (default: 24)

# Examples for other Gemini model configurations:

# For Gemini 1.5 models:
# BIG_MODEL="gemini-1.5-pro-latest"
# SMALL_MODEL="gemini-1.5-flash-latest"

# For experimental models:
# BIG_MODEL="gemini-2.0-flash-exp"
# SMALL_MODEL="gemini-1.5-flash-latest"

# Note: Get your Gemini API key from https://aistudio.google.com/app/apikey
# This project only supports Gemini API for Claude-compatible requests.
